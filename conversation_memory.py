#!/usr/bin/env python3
"""
Conversation Memory Manager for Multi-LLM Chatbot

This module provides:
- Intelligent conversation buffer management
- Context-size aware truncation & summarization
- Global persona rules (imported from system_prompts)
- Strict formatting enforcement for assistant messages
"""

import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timezone

# Use TikToken for accurate tokenization
try:
    import tiktoken
except ImportError:
    tiktoken = None

# Import strict global persona + formatting enforcement
from system_prompts import get_master_system_prompt, enforce_formatting, STUDY_MODE_SYSTEM_PROMPT, REASONING_MODE_SYSTEM_PROMPT


@dataclass
class ConversationMessage:
    """Represents a single message in the conversation."""
    role: str  # "system", "user", "assistant"
    content: str
    timestamp: datetime
    token_count: int = 0
    is_pinned: bool = False
    is_summary: bool = False


@dataclass
class ModelConfig:
    """Configuration for a specific LLM model."""
    name: str
    max_tokens: int
    reserve_tokens: int = 1000
    summary_threshold: float = 0.7


class TokenCounter:
    """Handles token counting using TikToken for accurate results."""

    def __init__(self):
        self.tokenizer = None
        if tiktoken:
            try:
                # Use GPT-4 tokenizer as it's most representative of modern LLMs
                self.tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4/ChatGPT tokenizer
            except Exception:
                self.tokenizer = None
        

    def count_tokens(self, text: str, model: Optional[str] = None) -> int:
        if not text:
            return 0
            
        if self.tokenizer:
            try:
                # Use TikToken for precise counting
                tokens = self.tokenizer.encode(text)
                return len(tokens)
            except Exception:
                pass
                
        # Fallback estimation if TikToken fails
        words = text.split()
        word_count = len(words)
        char_count = len(text)
        
        # Aggressive estimation to match real tokenization
        base_multiplier = 2.2  # Slightly higher for safety
        estimated_tokens = int(word_count * base_multiplier)
        
        # Additional tokens for special characters
        special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
        estimated_tokens += special_chars // 2
        
        # Long word penalty
        long_words = sum(1 for word in words if len(word) > 6)
        estimated_tokens += long_words
        
        # Character-based adjustment for very long text
        if char_count > 1000:
            estimated_tokens += char_count // 40
        
        return max(1, estimated_tokens)

    def count_message_tokens(self, message: Dict[str, str], model: Optional[str] = None) -> int:
        return self.count_tokens(message.get("content", ""), model) + 10


class ConversationSummarizer:
    """Generates compact summaries of conversation segments."""

    @staticmethod
    def create_summary(messages: List[ConversationMessage]) -> str:
        if not messages:
            return ""
        user_msgs = [m for m in messages if m.role == "user"]
        asst_msgs = [m for m in messages if m.role == "assistant"]

        parts = []
        if user_msgs:
            topics = []
            for msg in user_msgs:
                snippet = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
                topics.append(snippet)
            if len(topics) == 1:
                parts.append(f"User asked: {topics[0]}")
            else:
                parts.append(f"User discussed: {'; '.join(topics[:3])}")
                if len(topics) > 3:
                    parts[-1] += f" and {len(topics)-3} other topics"

        if asst_msgs:
            if len(asst_msgs) == 1:
                snippet = asst_msgs[0].content[:150]
                if len(asst_msgs[0].content) > 150:
                    snippet += "..."
                parts.append(f"Assistant responded: {snippet}")
            else:
                parts.append(f"Assistant provided {len(asst_msgs)} detailed responses")

        return " | ".join(parts)


class ConversationMemoryManager:
    """Main conversation memory manager with strict formatting."""

    MODEL_CONFIGS = {
        "meta/llama-4-maverick-17b-128e-instruct": ModelConfig("Llama 4 Maverick", 1_000_000),
        "deepseek-ai/deepseek-r1": ModelConfig("DeepSeek R1", 128_000),
        "qwen/qwen2.5-coder-32b-instruct": ModelConfig("Qwen 2.5 Coder", 32_000),
        "qwen/qwen3-coder-480b-a35b-instruct": ModelConfig("Qwen3 Coder 480B", 256_000),
        "deepseek-ai/deepseek-v3.1": ModelConfig("DeepSeek V3.1", 128_000),
        "deepseek-ai/deepseek-v3.2": ModelConfig("DeepSeek V3.2", 128_000),
        "openai/gpt-oss-120b": ModelConfig("GPT OSS", 128_000),
        "qwen/qwen3-235b-a22b:free": ModelConfig("Qwen3 235B", 131_000),
        "google/gemma-3-27b-it:free": ModelConfig("Gemma 3", 96_000),
        "moonshotai/kimi-k2-thinking": ModelConfig("Kimi K2 Thinking", 256_000),
    }

    def __init__(self, session_id: str = "default", fmt: str = "markdown"):
        self.session_id = session_id
        self.messages: List[ConversationMessage] = []
        self.token_counter = TokenCounter()
        self.summarizer = ConversationSummarizer()
        self.current_model: Optional[str] = None
        self.format = fmt  # "markdown" (default), "plaintext", "json", "yaml"

        # Add pinned global system rules
        self._add_system_prompt()

    def _add_system_prompt(self, is_study_mode: bool = False, is_reasoning_mode: bool = False):
        if is_reasoning_mode:
            system_content = REASONING_MODE_SYSTEM_PROMPT
        elif is_study_mode:
            system_content = STUDY_MODE_SYSTEM_PROMPT
        else:
            system_content = get_master_system_prompt()
            
        system_msg = ConversationMessage(
            role="system",
            content=system_content,
            timestamp=datetime.now(timezone.utc),
            token_count=self.token_counter.count_tokens(system_content),
            is_pinned=True,
        )
        self.messages.append(system_msg)

    def set_study_mode(self, enabled: bool = True):
        """Switches the system prompt to Study Mode or reverts to default."""
        # Remove existing system prompt
        self.messages = [m for m in self.messages if m.role != "system" or not m.is_pinned]
        # Add new prompt
        self._add_system_prompt(is_study_mode=enabled)
        # Ensure system prompt is first
        sys_msg = self.messages.pop(-1) # _add_system_prompt appends to end
        self.messages.insert(0, sys_msg)

    def set_reasoning_mode(self, enabled: bool = True):
        """Switches the system prompt to Reasoning Mode (with stealth rules) or reverts to default."""
        # Remove existing system prompt
        self.messages = [m for m in self.messages if m.role != "system" or not m.is_pinned]
        # Add new prompt with REASONING_RULES at the top
        self._add_system_prompt(is_reasoning_mode=enabled)
        # Ensure system prompt is first
        sys_msg = self.messages.pop(-1)
        self.messages.insert(0, sys_msg)

    def set_mode(self, use_study_mode: bool = False, use_reasoning_mode: bool = False):
        """
        Unified mode setter that handles both Study and Reasoning modes together.
        Priority: Reasoning > Study > Default
        This prevents modes from overwriting each other.
        """
        # Remove existing system prompt
        self.messages = [m for m in self.messages if m.role != "system" or not m.is_pinned]
        
        # Add appropriate prompt based on active modes
        self._add_system_prompt(is_study_mode=use_study_mode, is_reasoning_mode=use_reasoning_mode)
        
        # Ensure system prompt is first
        sys_msg = self.messages.pop(-1)
        self.messages.insert(0, sys_msg)

    def set_model(self, model_name: str):
        self.current_model = model_name
        if model_name not in self.MODEL_CONFIGS:
            self.MODEL_CONFIGS[model_name] = ModelConfig(f"Unknown-{model_name}", 32_000)

    def get_model_config(self) -> ModelConfig:
        if not self.current_model:
            return ModelConfig("Default", 32_000)
        return self.MODEL_CONFIGS.get(self.current_model, ModelConfig("Default", 32_000))

    def add_message(self, role: str, content: str, is_pinned: bool = False) -> ConversationMessage:
        if content is None:
            content = ""
        
        # Strictly enforce formatting only for assistant outputs
        if role == "assistant":
            content = enforce_formatting(content, self.format)

        token_count = self.token_counter.count_tokens(content, self.current_model)
        
        msg = ConversationMessage(
            role=role,
            content=content or "",  # Ensure content is never None
            timestamp=datetime.now(timezone.utc),
            token_count=token_count,
            is_pinned=is_pinned,
        )
        self.messages.append(msg)
        
        self._manage_buffer_size()
        return msg

    def _calculate_total_tokens(self) -> int:
        return sum(m.token_count for m in self.messages)

    def _manage_buffer_size(self):
        cfg = self.get_model_config()
        max_tokens = cfg.max_tokens - cfg.reserve_tokens
        total = self._calculate_total_tokens()
        if total <= max_tokens:
            return
        threshold = int(max_tokens * cfg.summary_threshold)
        if total > threshold:
            self._truncate_with_summary(max_tokens)
        else:
            self._simple_truncate(max_tokens)

    def _simple_truncate(self, max_tokens: int):
        total = self._calculate_total_tokens()
        removable = [i for i, m in enumerate(self.messages) if not m.is_pinned and m.role != "system"]
        for i in sorted(removable):
            if total <= max_tokens:
                break
            removed = self.messages.pop(i)
            total -= removed.token_count
            removable = [idx - 1 if idx > i else idx for idx in removable if idx != i]

    def _truncate_with_summary(self, max_tokens: int):
        total = self._calculate_total_tokens()
        pinned = [m for m in self.messages if m.is_pinned or m.role == "system"]
        unpinned = [m for m in self.messages if not m.is_pinned and m.role != "system"]

        if len(unpinned) <= 2:
            self._simple_truncate(max_tokens)
            return

        pinned_tokens = sum(m.token_count for m in pinned)
        target_unpinned = max_tokens - pinned_tokens

        keep, summarize, current = [], [], 0
        for m in reversed(unpinned):
            if current + m.token_count <= target_unpinned:
                keep.insert(0, m)
                current += m.token_count
            else:
                summarize.insert(0, m)

        if summarize:
            summary_text = self.summarizer.create_summary(summarize)
            summary_msg = ConversationMessage(
                role="system",
                content=f"[CONVERSATION SUMMARY] {summary_text}",
                timestamp=datetime.now(timezone.utc),
                token_count=self.token_counter.count_tokens(summary_text),
                is_summary=True,
                is_pinned=True,
            )
            self.messages = pinned + [summary_msg] + keep

    def get_conversation_buffer(self) -> List[Dict[str, Any]]:
        buf = []
        for m in self.messages:
            entry: Dict[str, Any] = {"role": m.role, "content": m.content}
            if getattr(m, "is_summary", False):
                entry["metadata"] = {"is_summary": True}
            buf.append(entry)
        return buf

    def get_conversation_stats(self) -> Dict[str, Any]:
        cfg = self.get_model_config()
        total = self._calculate_total_tokens()
        
        # Calculate displayed tokens (exclude system messages from user count)
        displayed = sum(m.token_count for m in self.messages if m.role != "system")
        
        # print(f"DEBUG: Stats calculation - {len(self.messages)} messages, {total} total tokens")
        
        return {
            "session_id": self.session_id,
            "current_model": self.current_model,
            "total_messages": len(self.messages),
            "total_tokens": total,
            "displayed_tokens": displayed,
            "max_tokens": cfg.max_tokens,
            "utilization_percent": round((total / cfg.max_tokens) * 100, 2),
            "pinned_messages": sum(1 for m in self.messages if m.is_pinned),
            "summary_messages": sum(1 for m in self.messages if getattr(m, "is_summary", False)),
        }

    def clear_conversation(self, keep_system_prompt: bool = True):
        if keep_system_prompt:
            self.messages = [m for m in self.messages if m.is_pinned and m.role == "system"]
        else:
            self.messages = []
            self._add_system_prompt()

    def export_conversation(self) -> Dict[str, Any]:
        return {
            "session_id": self.session_id,
            "current_model": self.current_model,
            "messages": [asdict(m) for m in self.messages],
            "stats": self.get_conversation_stats(),
        }

    def import_conversation(self, data: Dict[str, Any]):
        self.session_id = data.get("session_id", "default")
        self.current_model = data.get("current_model")
        self.messages = []
        for m in data.get("messages", []):
            if isinstance(m.get("timestamp"), str):
                m["timestamp"] = datetime.fromisoformat(m["timestamp"].replace("Z", "+00:00"))
            self.messages.append(ConversationMessage(**m))


# Global memory managers
_memory_managers: Dict[str, ConversationMemoryManager] = {}


def get_memory_manager(session_id: str = "default", fmt: str = "markdown") -> ConversationMemoryManager:
    if session_id not in _memory_managers:
        _memory_managers[session_id] = ConversationMemoryManager(session_id, fmt=fmt)
    return _memory_managers[session_id]


def cleanup_old_sessions(max_sessions: int = 100):
    if len(_memory_managers) > max_sessions:
        oldest = sorted(_memory_managers.keys())[: len(_memory_managers) - max_sessions]
        for sid in oldest:
            del _memory_managers[sid]